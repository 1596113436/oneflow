#include "oneflow/core/framework/op_generated.h"

namespace oneflow {

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutOp::InferLogicalTensorDesc(user_op::InferContext *ctx) {
      const user_op::TensorDesc& x_desc = ctx->InputTensorDesc("x", 0);
      const user_op::TensorDesc& mask_desc = ctx->InputTensorDesc("mask", 0);
      CHECK_OR_RETURN(x_desc.shape() == mask_desc.shape());
      *ctx->OutputShape("y", 0) = x_desc.shape();
      *ctx->OutputIsDynamic("y", 0) = x_desc.is_dynamic();
      *ctx->OutputShape("softmax_y", 0) = x_desc.shape();
      *ctx->OutputIsDynamic("softmax_y", 0) = x_desc.is_dynamic();
      return Maybe<void>::Ok();
    }

/*static*/ Maybe<void> FusedScaleMaskSoftmaxDropoutOp::InferPhysicalTensorDesc(user_op::InferContext* ctx) {return InferLogicalTensorDesc(ctx);}

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutOp::GetSbp(user_op::SbpContext *ctx) {
      const user_op::TensorDesc& x_tensor = ctx->LogicalTensorDesc4InputArgNameAndIndex("x", 0);
      CHECK_GE_OR_RETURN(x_tensor.shape().NumAxes(), 2);
      FOR_RANGE(int64_t, axis, 0, x_tensor.shape().NumAxes() - 2) {
        ctx->NewBuilder()
            .Split(user_op::OpArg("x", 0), axis)
            .Split(user_op::OpArg("mask", 0), axis)
            .Split(user_op::OpArg("dropout_mask", 0), axis)
            .Split(user_op::OpArg("y", 0), axis)
            .Split(user_op::OpArg("softmax_y", 0), axis)
            .Build();
      }
      return Maybe<void>::Ok();
    }

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutOp::ModifyInputArg(GetInputArgModifier GetInputArgModifierFn, const user_op::UserOpConfWrapper &conf) {
      user_op::InputArgModifier* mask_modifier = GetInputArgModifierFn("mask", 0);
      user_op::InputArgModifier* dropout_mask_modifier = GetInputArgModifierFn("dropout_mask", 0);
      CHECK_OR_RETURN(mask_modifier != nullptr);
      CHECK_OR_RETURN(dropout_mask_modifier != nullptr);
      mask_modifier->set_requires_grad(false);
      dropout_mask_modifier->set_requires_grad(false);
      return Maybe<void>::Ok();
    }

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutOp::InferDataType(user_op::InferContext *ctx) {
      const user_op::TensorDesc& x_desc = ctx->InputTensorDesc("x", 0);
      const user_op::TensorDesc& mask_desc = ctx->InputTensorDesc("mask", 0);
      CHECK_OR_RETURN(mask_desc.data_type() == DataType::kInt8);
      *ctx->OutputDType("y", 0) = x_desc.data_type();
      *ctx->OutputDType("softmax_y", 0) = x_desc.data_type();
      return Maybe<void>::Ok();
    }

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutGradOp::InferLogicalTensorDesc(user_op::InferContext *ctx) {
      const user_op::TensorDesc& softmax_y_desc = ctx->InputTensorDesc("softmax_y", 0);
      const user_op::TensorDesc& dy_desc = ctx->InputTensorDesc("dy", 0);
      const user_op::TensorDesc& mask_desc = ctx->InputTensorDesc("mask", 0);
      CHECK_EQ_OR_RETURN(dy_desc.shape(), softmax_y_desc.shape());
      CHECK_OR_RETURN(dy_desc.shape() == mask_desc.shape());
      user_op::TensorDesc* dx_desc = ctx->OutputTensorDesc("dx", 0);
      *dx_desc->mut_shape() = dy_desc.shape();
      *dx_desc->mut_is_dynamic() = dy_desc.is_dynamic();
      return Maybe<void>::Ok();
    }

/*static*/ Maybe<void> FusedScaleMaskSoftmaxDropoutGradOp::InferPhysicalTensorDesc(user_op::InferContext* ctx) {return InferLogicalTensorDesc(ctx);}

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutGradOp::GetSbp(user_op::SbpContext *ctx) {
      const user_op::TensorDesc& dy_tensor = ctx->LogicalTensorDesc4InputArgNameAndIndex("dy", 0);
      CHECK_GE_OR_RETURN(dy_tensor.shape().NumAxes(), 2);
      FOR_RANGE(int64_t, axis, 0, dy_tensor.shape().NumAxes() - 2) {
        ctx->NewBuilder()
            .Split(user_op::OpArg("softmax_y", 0), axis)
            .Split(user_op::OpArg("dy", 0), axis)
            .Split(user_op::OpArg("mask", 0), axis)
            .Split(user_op::OpArg("dropout_mask", 0), axis)
            .Split(user_op::OpArg("dx", 0), axis)
            .Build();
      }
      return Maybe<void>::Ok();
    }

/* static */ Maybe<void> FusedScaleMaskSoftmaxDropoutGradOp::InferDataType(user_op::InferContext *ctx) {
      const user_op::TensorDesc& softmax_y_desc = ctx->InputTensorDesc("softmax_y", 0);
      const user_op::TensorDesc& dy_desc = ctx->InputTensorDesc("dy", 0);
      const user_op::TensorDesc& mask_desc = ctx->InputTensorDesc("mask", 0);
      CHECK_OR_RETURN(dy_desc.data_type() == softmax_y_desc.data_type());
      CHECK_OR_RETURN(mask_desc.data_type() == DataType::kInt8);
      user_op::TensorDesc* dx_desc = ctx->OutputTensorDesc("dx", 0);
      *dx_desc->mut_data_type() = dy_desc.data_type();
      return Maybe<void>::Ok();
    }

